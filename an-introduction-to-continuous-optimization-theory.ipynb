{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cd862f7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.001938,
     "end_time": "2023-09-23T13:34:22.006752",
     "exception": false,
     "start_time": "2023-09-23T13:34:22.004814",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Continuous Optimization Theory**\n",
    "Continuous optimization is used in understanding the properties of the curve of problem function, estimating the candidates of local minimizer, local minimum and if lucky, to estimate the global minimum. The understanding of the function curve helps us in solving various machine learning problems using the mathematical tools.\n",
    "Under non-linear programming continuous optimization is divided into 2 parts i.e. Unconstrained Optimization and Constrained Optimization. The difference between these two is very basic in nature. In constrained optimization the given function is accompanied with few constraints, these define the limits for the feasible region (a region where the solution exists). On the other hand, unconstrained optimization do not have constraints but only the  function itself. The job of both is to find out the minimum of the given function. \n",
    "### Local Minimum\n",
    "The local minimum is very important in finding the solution for a function in respect of machine learning as local minimizer be the solution of function but it may or may not be the optimial solution of the function.\n",
    "To find out the local minimum we have to first understand the nature of the curve of function. The curve of function can be convex or concave. The candidates of local minimum exists in a convex funtion and thus to find out the solution for a convex function is less complex than a concave funciton. The strict convex function has a unique local minimum and it also the global minimum (i.e. the optimal solution).\n",
    "### **Unconstrained Optimization Theory**\n",
    "The unconstarined minimization problem for a function f(x) is calculating the minimum of it where x belongs to Real number.\n",
    "This problem is called unconstrained nonlinear program. The function f(x) is an objective function with domain as n-dimensional Real vector space and with co-domain as 1-dimensional Real vector space.\n",
    "The following conditions can help to identify the minimum of the function.\n",
    "#### Necessary Optimality Condition of First Order\n",
    "Let a point x* be a local minimizer of min f(x) then the following condition holds: \n",
    "\n",
    "the first order partial derivative of f(x*) is equal to 0. However, this condition is not sufficient but it tells us about the possible candidates of the local minimum. Therefore, we proceed with the foloowing condition.\n",
    "#### Necessary Optimality Condition of Second Order\n",
    "Let a point x be a local minimum of min f(x), then the following conditions hold:\n",
    "\n",
    "fisrt order partial derivative of f(x) = 0 and second order partial derivative of f(x) is positive semidefinite. Although, all the local minimizers satisfy these conditions, but there may be more additional points that may satisfy these conditions without being a local minimizers.\n",
    "\n",
    "* Positive Semidefinte: a function which is PSD, it has non-negative eigen values from its Hessian matrix and therefore the function is convex but not a strict convex function.\n",
    "* Hessian Matrix:  the second order partial derivative of a scalar-valued function is called a hessain matrix.\n",
    "\n",
    "#### Sufficient Optimality Condition of Second Order\n",
    "If a point x* satisfies the following condtions: \n",
    "\n",
    "fisrt order partial derivative of f(x*) = 0 and second order partial derivative of f(x*) is positive definite. Then x* is the strict local minimizer of min f(x).\n",
    "\n",
    "* Positive Definte: a function which has all positive eigen values from its Hessian matrix and therefore the function is strict convex.\n",
    "\n",
    "Note: If sufficient condition is satisfied, it confirms the presence of local minimizer. However, there exists some local minimizers that do not satisfy the sufficient condition and therefore if the sufficient condition is not satisfied, we may not conclude that the popint is not a local minimizer.\n",
    "#### Necessary and Sufficient Optimality Condition of First Order for Convex Functions\n",
    "Let the function f be convex. If and only if the first order partial derivative of f(x*) = 0 then x* is a local and global minimum of min f(x).\n",
    "### **Constrained Optimization Theory**\n",
    "The constarined minimization problem for a function f(x) is calculating the minimum of it where x belongs to Real number and with given equality constriants and inequality constraints, such that g(x) = 0 and h(x) >= 0. The functions g and h are mapped from multi-dimensional (domain) to multi-dimensional (co-domain) real vector space. \n",
    "This problem is called constrained nonlinear program (NLP). The function f(x) is an objective function with domain as n-dimensional Real vector space and with co-domain as 1-dimensional Real vector space.\n",
    "The following conditions can help to identify the minimum of the function.\n",
    "#### Necessary Optimality Condition of First Order\n",
    "Let x* be a local minimizer of (NLP), and let constraint qualification holds at point x*, then the following conditions hold:\n",
    "\n",
    "the x* is the feasible point i.e. g(x*) = 0 and h(x*) >= 0 and,\n",
    "the inner product of first order partial derivative of f(x*) and d is >= 0. \n",
    "\n",
    "Here, d is the direction of the possible feasible set and d belongs to the Linearized cone, in the direction d the given constraints hold.\n",
    "\n",
    "* Feasible set: The set of all points that satisfies the equality and ineaquality constraints.\n",
    "* Linearized Cone: It gives the direction for a feasible point such that the point can satisfy the gradients of equality and inequality constraints in that direction.\n",
    "* Tangent Cone: It helps to determine the feasible directions for improving or decreasing the objective function while satisfying constraints. The tangent cone is difficult to work with therefore Linearized cone is considered.\n",
    "* Constraint Qualification: holds at a point x if at that point the linearized cone is equal to the tangent cone.\n",
    "\n",
    "#### Necessary Optimality Condition of Second Order\n",
    "Let x* be a local minimizer of (NLP), and let constraint qualification holds at point x*, then the following conditions hold:\n",
    "\n",
    "the x* is a stationary point and,\n",
    "the hessian matrix of f(x*) is positive semidefinite.\n",
    "\n",
    "* Stationary Point: If a feasible point at which the constraint qualification holds and the gradient of f for that point is greater than 0 in all the directions of linearized cone.\n",
    "\n",
    "#### Sufficient Optimality Condition of Second Order\n",
    "If a stationary point x of (NLP) satisfies the following conditions:\n",
    "\n",
    "the hessian matrix of f(x*) is positive definite then it is a strict local minimum of NLP.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2.817912,
   "end_time": "2023-09-23T13:34:22.227144",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-09-23T13:34:19.409232",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
